import numpy as np
from typing import List
import torch
from sentence_transformers import SentenceTransformer
import logging 
 
logging.basicConfig(level=logging.INFO)
 
class RecipeEmbedder:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ä–µ—Ü–µ–ø—Ç–æ–≤.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.
    """
    
    def __init__(self, model_name: str = "sentence-transformers/distiluse-base-multilingual-cased"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace
        """
        logging.info(f"model_name: {model_name}")
        

        device = "cuda" if torch.cuda.is_available() else "cpu"
            
        logging.info(f"device: {device}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = SentenceTransformer(model_name, device=device)
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.embedding_dim}")
    
    def encode_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        –ö–æ–¥–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –ú–∞—Ç—Ä–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ shape=(len(texts), embedding_dim)
        """
        print(f"üîÑ –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        processed_texts = []
        for text in texts:
            if not text or not text.strip():
                processed_texts.append("[EMPTY]")  # –ü–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä –¥–ª—è –ø—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            else:
                processed_texts.append(text.strip())
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings = self.model.encode(
            processed_texts,
            batch_size=batch_size,
            show_progress_bar=True,
            normalize_embeddings=True,  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            convert_to_numpy=True
        )
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {embeddings.shape[0]} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        return embeddings
    
    def encode_query(self, query: str) -> np.ndarray:
        """
        –ö–æ–¥–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
        
        Args:
            query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            
        Returns:
            –í–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞
        """
        if not query.strip():
            query = "[EMPTY]"
        
        embedding = self.model.encode(
            [query.strip()],
            normalize_embeddings=True,
            convert_to_numpy=True
        )
        
        return embedding[0]
    
    def get_similarity(self, text1: str, text2: str) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏.
        
        Args:
            text1: –ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
            text2: –í—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞ (0-1)
        """
        emb1 = self.encode_query(text1)
        emb2 = self.encode_query(text2)
        
        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–≤–µ–∫—Ç–æ—Ä—ã —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã)
        similarity = np.dot(emb1, emb2)
        return float(similarity)