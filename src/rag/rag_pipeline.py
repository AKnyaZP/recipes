
# ...existing code...
"""
–û—Å–Ω–æ–≤–Ω–æ–π RAG –ø–∞–π–ø–ª–∞–π–Ω - —Å–≤—è–∑—ã–≤–∞–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã.
"""

from typing import List, Dict, Any, Optional
import time
from pathlib import Path
import os
from logging import getLogger

from data.data_loader import (
    load_povarenok_data,
    prepare_documents,
    save_processed_data,
    load_processed_data,
)
from embeddings.embeddings import RecipeEmbedder
from store.vector_store import FAISSVectorStore
from rag.hybrid_search import HybridSearch
from llm.llm import RecipeLLM

logger = getLogger(__name__)
logger.setLevel("DEBUG")


def _to_list_of_dicts(obj) -> Optional[List[Dict[str, Any]]]:
    """
    –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–∏–≤–µ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç –∫ —Å–ø–∏—Å–∫—É —Å–ª–æ–≤–∞—Ä–µ–π.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: list[dict], dict (–∫–ª—é—á -> list), HF Dataset/DatasetDict, pandas DataFrame –∏ —Ç.–¥.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–≤–µ—Å—Ç–∏.
    """
    if obj is None:
        return None

    # already list of dicts?
    if isinstance(obj, list):
        return obj

    # dict of lists -> convert by zipping
    if isinstance(obj, dict):
        # –µ—Å–ª–∏ —Å–ª–æ–≤–∞—Ä—å –≤–∏–¥–∞ {'train': [...]}
        # –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–µ—Ä–≤—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é –∑–∞–ø–∏—Å—å —Å–æ —Å–ø–∏—Å–∫–æ–º —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–ª–æ–≤–∞—Ä–µ–π
        # –∏–ª–∏ –∫–ª—é—á —Å–æ —Å–ø–∏—Å–∫–æ–º –¥–ª–∏–Ω–Ω–æ–π > 0
        for k, v in obj.items():
            if isinstance(v, list) and v and isinstance(v[0], dict):
                return v
        # –µ—Å–ª–∏ dict of lists (columns) -> zip to records
        list_values = [v for v in obj.values() if isinstance(v, list)]
        if list_values:
            length = len(list_values[0])
            try:
                keys = list(obj.keys())
                records = []
                for i in range(length):
                    rec = {}
                    for k in keys:
                        val = obj[k]
                        rec[k] = val[i] if isinstance(val, list) and i < len(val) else None
                    records.append(rec)
                return records
            except Exception:
                pass
        return None

    # objects with to_pandas / to_dict / to_list behaviour (HF Dataset, pandas.DataFrame)
    try:
        # HF Dataset / DatasetDict -> try iterate or use to_dict
        if hasattr(obj, "to_dict"):
            d = obj.to_dict()
            # if to_dict returns dict of lists -> convert
            if isinstance(d, dict):
                # DatasetDict returns {split: Dataset}, handle that
                # if values are lists -> zip to records
                for v in d.values():
                    if isinstance(v, list) and v and isinstance(v[0], dict):
                        return v
                # if d itself is columns -> zip
                list_values = [v for v in d.values() if isinstance(v, list)]
                if list_values:
                    length = len(list_values[0])
                    keys = list(d.keys())
                    records = []
                    for i in range(length):
                        rec = {}
                        for k in keys:
                            val = d[k]
                            rec[k] = val[i] if isinstance(val, list) and i < len(val) else None
                        records.append(rec)
                    return records

        # try pandas
        if hasattr(obj, "to_records") or hasattr(obj, "to_dict"):
            try:
                # pandas.DataFrame -> to_dict(orient='records')
                recs = obj.to_dict(orient="records")
                if isinstance(recs, list):
                    return recs
            except Exception:
                pass

        # fallback: try to iterate and coerce to list
        try:
            lst = list(obj)
            # ensure elements are dicts
            if lst and isinstance(lst[0], dict):
                return lst
            # if elements are tuples/lists and correspond to columns, try to convert
        except Exception:
            pass
    except Exception:
        pass

    return None


class RecipeRAGPipeline:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å RAG –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ—Ü–µ–ø—Ç–æ–≤.
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö, –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é, –ø–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é.
    """

    def __init__(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç RAG –ø–∞–π–ø–ª–∞–π–Ω.
        """
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Ä–µ—Ü–µ–ø—Ç–æ–≤")

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞
        self.embedder: Optional[RecipeEmbedder] = None
        self.vector_store: Optional[FAISSVectorStore] = None
        self.hybrid_search: Optional[HybridSearch] = None
        self.llm: Optional[RecipeLLM] = None
        self.documents: List[Dict[str, Any]] = []

        print("‚úÖ RAG –ø–∞–π–ø–ª–∞–π–Ω —Å–æ–∑–¥–∞–Ω")

    def setup_embeddings(self):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.
        """
        print("\nüìù –®–∞–≥ 1: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
        self.embedder = RecipeEmbedder("sentence-transformers/distiluse-base-multilingual-cased")

    def load_and_process_data(self, max_recipes: int = None):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ä–µ—Ü–µ–ø—Ç–æ–≤.

        Args:
            max_recipes: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ—Ü–µ–ø—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        print(f"\nüìÇ –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ª–∏–º–∏—Ç: {max_recipes or '–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π'})")

        # compute processed file path at repo root /data/processed_recipes.json
        processed_file = Path(__file__).resolve().parents[2] / "data" / "processed_recipes.json"

        # 1) –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–æ—Ç–æ–≤—ã–π processed —Ñ–∞–π–ª (–µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å)
        processed = None
        if processed_file.exists():
            try:
                print(f"üìã –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏: {processed_file}")
                processed = load_processed_data(str(processed_file))
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ processed_file: {e}")
                processed = None

        # 2) –ï—Å–ª–∏ processed –ø—É—Å—Ç –∏–ª–∏ None -> –∑–∞–≥—Ä—É–∑–∏–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–µ–º
        if not processed:
            print("‚ÑπÔ∏è processed data –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç/–ø—É—Å—Ç—ã ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
            hf_token = os.getenv("HF_TOKEN")

            # –í—ã–∑–æ–≤ load_povarenok_data: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–∏–≥–Ω–∞—Ç—É—Ä
            raw_recipes = None
            try:
                # try signature with max_recipes kwarg
                raw_recipes = load_povarenok_data(max_recipes=max_recipes, use_auth_token=hf_token)
            except TypeError:
                try:
                    # common alternative: load_povarenok_data(dataset_id, max_recipes=...)
                    raw_recipes = load_povarenok_data("rogozinushka/povarenok-recipes", max_recipes=max_recipes, use_auth_token=hf_token)
                except TypeError:
                    try:
                        # try simple call without args
                        raw_recipes = load_povarenok_data()
                    except Exception as e:
                        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–∑–≤–∞—Ç—å load_povarenok_data –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏: {e}")
                        raw_recipes = None
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö (second attempt): {e}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö (first attempt): {e}")

            if not raw_recipes:
                raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ—Ü–µ–ø—Ç—ã (raw_recipes –ø—É—Å—Ç—ã–µ). –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–ª–∏ HF_TOKEN.")

            print("‚öôÔ∏è –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
            try:
                # prepare_documents –º–æ–∂–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä max_recipes
                try:
                    processed = prepare_documents(raw_recipes, max_recipes)
                except TypeError:
                    processed = prepare_documents(raw_recipes)
            except Exception as e:
                raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ prepare_documents: {e}")

            if not processed:
                raise RuntimeError("prepare_documents –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            try:
                # try signature save_processed_data(processed, path) or save_processed_data(path, processed)
                try:
                    save_processed_data(processed, str(processed_file))
                except TypeError:
                    save_processed_data(str(processed_file), processed)
                print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {processed_file}")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å processed data –Ω–∞ –¥–∏—Å–∫: {e}")

        # 3) –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º processed –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
        normalized = _to_list_of_dicts(processed)
        if normalized is None:
            # –µ—Å–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤—Å—ë –µ—â—ë –Ω–µ –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –ø—Ä–æ–±—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ load_processed_data –µ—Å–ª–∏ –Ω–µ –¥–µ–ª–∞–ª–∏ —Ä–∞–Ω–µ–µ
            if not processed_file.exists():
                raise RuntimeError("Processed data –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –∏ –∫—ç—à –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.")
            else:
                # –ø–æ–ø—ã—Ç–∫–∞ –µ—â—ë —Ä–∞–∑ –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ load_processed_data
                try:
                    reloaded = load_processed_data(str(processed_file))
                    normalized = _to_list_of_dicts(reloaded)
                except Exception:
                    normalized = None

            if normalized is None:
                raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–≤–µ—Å—Ç–∏ processed data –∫ —Å–ø–∏—Å–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –¢–∏–ø –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞: {type(processed)}")

        # –ø—Ä–∏–º–µ–Ω—è–µ–º –ª–∏–º–∏—Ç, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if max_recipes and len(normalized) > max_recipes:
            normalized = normalized[:max_recipes]
            print(f"üî¨ –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {max_recipes} —Ä–µ—Ü–µ–ø—Ç–æ–≤")

        self.documents = normalized
        print(f"‚úÖ –ì–æ—Ç–æ–≤–æ –∫ —Ä–∞–±–æ—Ç–µ —Å {len(self.documents)} —Ä–µ—Ü–µ–ø—Ç–∞–º–∏")

    def build_vector_index(self, force_rebuild: bool = False):
        """
        –°—Ç—Ä–æ–∏—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å FAISS.

        Args:
            force_rebuild: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        """
        print("\nüóÑÔ∏è –®–∞–≥ 3: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞")

        if not self.embedder:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é setup_embeddings()")

        if not self.documents or len(self.documents) == 0:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é load_and_process_data(); –Ω–∞–π–¥–µ–Ω–æ 0 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.vector_store = FAISSVectorStore(self.embedder.embedding_dim)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
        index_path = Path(__file__).resolve().parents[2] / "data" / "faiss_index"

        if index_path.exists() and not force_rebuild:
            print("üìÇ –ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å, –∑–∞–≥—Ä—É–∂–∞–µ–º...")
            try:
                self.vector_store.load(str(index_path))
                return
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
                print("üî® –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å...")

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        print("üîÑ –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
        texts = [doc.get("full_text", "") for doc in self.documents]
        embeddings = self.embedder.encode_texts(texts)

        print("üèóÔ∏è –°—Ç—Ä–æ–∏–º FAISS –∏–Ω–¥–µ–∫—Å...")
        self.vector_store.build_index(embeddings, self.documents)

        print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å...")
        self.vector_store.save(str(index_path))

        print("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤")

    def setup_hybrid_search(self):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫.
        """
        print("\nüîç –®–∞–≥ 4: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")

        if not self.vector_store or not self.documents:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å")

        self.hybrid_search = HybridSearch(self.vector_store, self.documents)
        print("‚úÖ –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –≥–æ—Ç–æ–≤")

    def setup_llm(self):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å.
        """
        print("\nü§ñ –®–∞–≥ 5: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏")

        self.llm = RecipeLLM("Qwen/Qwen2.5-1.5B-Instruct")

        # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
        model_info = self.llm.get_model_info()
        print(f"üìã –ú–æ–¥–µ–ª—å: {model_info.get('model_name', 'unknown')}")
        print(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: ~{model_info.get('num_parameters', 0):,}")
        print("‚úÖ LLM –≥–æ—Ç–æ–≤–∞")

    def initialize_full_pipeline(self, max_recipes: int = None, force_rebuild: bool = False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω –∑–∞ –æ–¥–∏–Ω –≤—ã–∑–æ–≤.

        Args:
            max_recipes: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ—Ü–µ–ø—Ç–æ–≤
            force_rebuild: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        """
        print("üöÄ –ü–æ–ª–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –ø–∞–π–ø–ª–∞–π–Ω–∞")

        start_time = time.time()

        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.setup_embeddings()
        self.load_and_process_data(max_recipes)
        self.build_vector_index(force_rebuild)
        self.setup_hybrid_search()
        self.setup_llm()

        elapsed = time.time() - start_time
        print(f"\n‚úÖ –ü–æ–ª–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed:.1f}—Å")
        print("üéâ RAG –ø–∞–π–ø–ª–∞–π–Ω –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")

    def search_recipes(self, query: str, k: int = 1) -> List[tuple]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Ä–µ—Ü–µ–ø—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É.
        """
        if not self.hybrid_search or not self.embedder:
            raise ValueError("–ü–∞–π–ø–ª–∞–π–Ω –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_embedding = self.embedder.encode_query(query)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
        results = self.hybrid_search.hybrid_search(
            query=query,
            query_embedding=query_embedding,
            k=k,
            alpha=0.6,
        )

        return results

    def ask(self, question: str) -> Dict[str, Any]:
        """
        –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ —Ä–µ—Ü–µ–ø—Ç–∞—Ö.
        """
        if not all([self.hybrid_search, self.embedder, self.llm]):
            raise ValueError("–ü–∞–π–ø–ª–∞–π–Ω –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        print(f"\n‚ùì –í–æ–ø—Ä–æ—Å: {question}")

        start_time = time.time()

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ—Ü–µ–ø—Ç–æ–≤
        search_results = self.search_recipes(question, k=5)

        search_time = time.time() - start_time

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = self.llm.generate_response(question, search_results)

        total_time = time.time() - start_time

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "question": question,
            "answer": response,
            "found_recipes": len(search_results) if search_results is not None else 0,
            "search_results": [
                {
                    "name": doc.get("name", ""),
                    "ingredients": doc.get("ingredients_text", ""),
                    "url": doc.get("url", ""),
                    "relevance_score": float(score),
                }
                for doc, score in (search_results or [])[:3]
            ],
            "timing": {"search_time": round(search_time, 3), "total_time": round(total_time, 3)},
        }

        print(f"üí¨ –û—Ç–≤–µ—Ç: {response}")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {total_time:.3f}—Å (–ø–æ–∏—Å–∫: {search_time:.3f}—Å)")

        return result

# ...existing code...
# if __name__ == "__main__":
#     # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞
#     print("üß™ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è RAG –ø–∞–π–ø–ª–∞–π–Ω–∞")
    
#     try:
#         # –°–æ–∑–¥–∞–µ–º –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞–π–ø–ª–∞–π–Ω
#         rag = RecipeRAGPipeline()
#         rag.initialize_full_pipeline(
#             max_recipes=100,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
#             force_rebuild=False
#         )
        
#         # –ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤
#         test_questions = [
#             "–ö–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å –±–æ—Ä—â?",
#             "–†–µ—Ü–µ–ø—Ç —Å–∞–ª–∞—Ç–∞ —Å –∫—É—Ä–∏—Ü–µ–π",
#             "–ß—Ç–æ –º–æ–∂–Ω–æ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å –∏–∑ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è?",
#             "–ü—Ä–æ—Å—Ç–æ–π –¥–µ—Å–µ—Ä—Ç —Å —è–±–ª–æ–∫–∞–º–∏"
#         ]
        
#         print("\n" + "="*50)
#         print("üîç –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï RAG –ü–ê–ô–ü–õ–ê–ô–ù–ê")
#         print("="*50)
        
#         for question in test_questions:
#             result = rag.ask(question)
            
#             print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ —Ä–µ—Ü–µ–ø—Ç–æ–≤: {result['found_recipes']}")
#             print("ü•ò –¢–æ–ø —Ä–µ—Ü–µ–ø—Ç–æ–≤:")
#             for i, recipe in enumerate(result['search_results'], 1):
#                 print(f"  {i}. {recipe['name']} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {recipe['relevance_score']:.3f})")
            
#             print("-" * 50)
        
#         print("\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        
#     except Exception as e:
#         print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
#         print("–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ:")
#         print("1. –î–∞—Ç–∞—Å–µ—Ç recipes.json –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ data/raw/")
#         print("2. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
#         print("3. –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–∏")